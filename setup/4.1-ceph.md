# 实验1. 基于docker的ceph基本操作实验

## 1. 启动ceph测试环境

启动docker
```
# service docker start
```

导入镜像
```
# docker load < /root/ceph.tar
# docker images
REPOSITORY          TAG                 IMAGE ID 
ceph/demo           latest              ed38ba053588
```

查看IP地址及网络
```
# ifconfig
```
这里假设IP地址为 172.17.0.2，网络为 172.17.0.0/16


启动 ceph测试环境（IP地址和网络需要替换为实际的值）
```
# docker run -d --net=host -v /etc/ceph:/etc/ceph -e MON_IP=172.17.0.2 -e CEPH_PUBLIC_NETWORK=172.17.0.0/16 ceph/demo
```

## 2. cephfs挂载

首先查看 key
```
# cat /etc/ceph/ceph.client.admin.keyring
AQBni81cnAC9GRAAzY2I9Wz+5gMdcY4ceGBcyA==(以实际输出为准)
```

挂载cephfs（IP地址和secret替换为上面实际输出的内容）
```
# mount -t ceph -o name=admin,secret=AQBni81cnAC9GRAAzY2I9Wz+5gMdcY4ceGBcyA== 172.17.0.2:/ /mnt/cephfs
```

查看挂载结果
```
# mount
```

## 3. rbd方式
还有一种方式是作为rbd来加载。rbd的模型：最外层是pool，相当于一块磁盘，默认的pool名字叫做rbd。每个pool里面可以有多个image，相当于文件夹。每个image可以映射成一个块设备，有了设备就可以加载它。

首先创建一个名为 test_pool 的 pool ：
```
# ceph osd pool create test_pool 128
```
128代表placement-group的数量。每个pg都是一个虚拟节点，将自己的数据存在不同的位置。这样一旦存储出现问题，pg就会选择新的存储，从而保证了自动高可用。运行这个命令就可以看到现在系统中的所有pool：
```
# ceph osd lspools
```
然后在 test_pool 这个pool里创建一个名为 test_image 的image：
```
# rbd create test_pool/test_image --size 128
```
size的单位是MB，所以这个 test_image image的大小为128M。运行下列命令可以看到 test_pool 的pool中的所有 test_image 和查看 test_image 的详细信息：
```
# rbd ls test_pool
# rbd info test_pool/test_image
```
